<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
<HEAD>
<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=us-ascii">
<META NAME="Generator" CONTENT="MS Exchange Server version 6.5.7036.0">
<TITLE>RE: [mpich2-maint] #403: mpiexec kills the remote login shell</TITLE>
</HEAD>
<BODY>
<!-- Converted from text/plain format -->

<P><FONT SIZE=2>&nbsp;Hi,<BR>
&nbsp; Enabling core dump will make it a lot easier to debug the problem.<BR>
&nbsp; Can you run mpiexec without any parameters (mpiexec just prints out its options)? Can you also try running mpiexec with the &quot;-localonly&quot; option (mpiexec -n 2 -localonly hostname)?<BR>
<BR>
Regards,<BR>
Jayesh<BR>
<BR>
-----Original Message-----<BR>
From: Yu-Cheng Chou [<A HREF="mailto:cycchou@ucdavis.edu">mailto:cycchou@ucdavis.edu</A>]<BR>
Sent: Wednesday, February 18, 2009 11:38 AM<BR>
To: Jayesh Krishna<BR>
Subject: Re: [mpich2-maint] #403: mpiexec kills the remote login shell<BR>
<BR>
Hi,<BR>
<BR>
At this point, I still have not found a way to enable the core dump generation on Korebot.<BR>
<BR>
Yucheng<BR>
<BR>
On Mon, Feb 16, 2009 at 7:43 AM, Jayesh Krishna &lt;jayesh@mcs.anl.gov&gt; wrote:<BR>
&gt;&nbsp; Hi,<BR>
&gt;&nbsp;&nbsp; I looked into the debug logs and the outputs show that mpiexec<BR>
&gt; reached the final exit() statement without any errors/crashes. Let me<BR>
&gt; take a look at the code and see how we can debug your problem further.<BR>
&gt; Have you enabled core dump generation on Korebot ?<BR>
&gt;<BR>
&gt; Regards,<BR>
&gt; Jayesh<BR>
&gt;<BR>
&gt; -----Original Message-----<BR>
&gt; From: Yu-Cheng Chou [<A HREF="mailto:cycchou@ucdavis.edu">mailto:cycchou@ucdavis.edu</A>]<BR>
&gt; Sent: Saturday, February 14, 2009 1:24 PM<BR>
&gt; To: Jayesh Krishna<BR>
&gt; Cc: mpich2-maint@mcs.anl.gov<BR>
&gt; Subject: Re: [mpich2-maint] #403: mpiexec kills the remote login shell<BR>
&gt;<BR>
&gt; Hi,<BR>
&gt;<BR>
&gt; I did what you suggested in the email.<BR>
&gt; After mpiexec finished the execution of hellow, the connection to<BR>
&gt; Korebot was gone, just like what happened before.<BR>
&gt; I attached smpd.log and mpiexec.log to this email.<BR>
&gt;<BR>
&gt; Thank you<BR>
&gt;<BR>
&gt;<BR>
&gt; On Thu, Feb 12, 2009 at 1:58 PM, Jayesh Krishna &lt;jayesh@mcs.anl.gov&gt; wrote:<BR>
&gt;&gt;&nbsp; Hi,<BR>
&gt;&gt;&nbsp;&nbsp; Can you try out the patch attached. The patch contains some extra<BR>
&gt;&gt; debug statements which will help us in narrowing down on your problem.<BR>
&gt;&gt;<BR>
&gt;&gt; Applying the patch<BR>
&gt;&gt; ---------------------<BR>
&gt;&gt; 1) change directory to top-level of mpich2 source<BR>
&gt;&gt; 2) patch -p0 &lt; mpich2_1_0_8_Korebot.patch<BR>
&gt;&gt; 3) Re-compile &amp; re-install MPICH2<BR>
&gt;&gt;<BR>
&gt;&gt;&nbsp;&nbsp; Now re-run smpd &amp; mpiexec in debug mode with a<BR>
&gt;&gt;&nbsp;&nbsp; simple mpi program,&nbsp; hellow.c (smpd -d &gt; smpd.log /<BR>
&gt;&gt;&nbsp;&nbsp; mpiexec -verbose -n 1 ./hellow &gt; mpiexec.log).<BR>
&gt;&gt;<BR>
&gt;&gt; Regards,<BR>
&gt;&gt; Jayesh<BR>
&gt;&gt;<BR>
&gt;&gt; -----Original Message-----<BR>
&gt;&gt; From: Yu-Cheng Chou [<A HREF="mailto:cycchou@ucdavis.edu">mailto:cycchou@ucdavis.edu</A>]<BR>
&gt;&gt; Sent: Thursday, February 12, 2009 1:33 PM<BR>
&gt;&gt; To: Jayesh Krishna<BR>
&gt;&gt; Subject: Re: [mpich2-maint] #403: mpiexec kills the remote login<BR>
&gt;&gt; shell<BR>
&gt;&gt;<BR>
&gt;&gt; Hi,<BR>
&gt;&gt; For the first question, I am not able to get the core dump for<BR>
&gt;&gt; mpiexec/hellow/ssh on the Korebot because of the limited memory on<BR>
&gt;&gt; the Korebot.<BR>
&gt;&gt;<BR>
&gt;&gt; For the second question, I can run such a program with fflush(stdout)<BR>
&gt;&gt; and<BR>
&gt;&gt; fflush(stderr) statements on the Korebot.<BR>
&gt;&gt;<BR>
&gt;&gt; Thank you<BR>
&gt;&gt;<BR>
&gt;&gt; On Thu, Feb 12, 2009 at 11:14 AM, Jayesh Krishna &lt;jayesh@mcs.anl.gov&gt;<BR>
&gt;&gt; wrote:<BR>
&gt;&gt;&gt; Hi,<BR>
&gt;&gt;&gt;&nbsp; I have yet to make the debug module (shouldn't take much time). The<BR>
&gt;&gt;&gt; answers to the questions in my prev email will help me to put in the<BR>
&gt;&gt;&gt; right debug statements.<BR>
&gt;&gt;&gt;<BR>
&gt;&gt;&gt; Regards,<BR>
&gt;&gt;&gt; Jayesh<BR>
&gt;&gt;&gt;<BR>
&gt;&gt;&gt; -----Original Message-----<BR>
&gt;&gt;&gt; From: Yu-Cheng Chou [<A HREF="mailto:cycchou@ucdavis.edu">mailto:cycchou@ucdavis.edu</A>]<BR>
&gt;&gt;&gt; Sent: Thursday, February 12, 2009 12:23 PM<BR>
&gt;&gt;&gt; To: Jayesh Krishna<BR>
&gt;&gt;&gt; Subject: Re: [mpich2-maint] #403: mpiexec kills the remote login<BR>
&gt;&gt;&gt; shell<BR>
&gt;&gt;&gt;<BR>
&gt;&gt;&gt; Hi,<BR>
&gt;&gt;&gt; Would you give me the debug module directly?<BR>
&gt;&gt;&gt; Thank you<BR>
&gt;&gt;&gt;<BR>
&gt;&gt;&gt; On Thu, Feb 12, 2009 at 10:15 AM, Jayesh Krishna<BR>
&gt;&gt;&gt; &lt;jayesh@mcs.anl.gov&gt;<BR>
&gt;&gt;&gt; wrote:<BR>
&gt;&gt;&gt;&gt; Hi,<BR>
&gt;&gt;&gt;&gt;&nbsp; Do you get a core dump of mpiexec/hellow/ssh ? (if yes, what does<BR>
&gt;&gt;&gt;&gt; it show<BR>
&gt;&gt;&gt;&gt; ?)<BR>
&gt;&gt;&gt;&gt;&nbsp; Can you run a simple non-MPI C program with fflush(stdout) &amp;<BR>
&gt;&gt;&gt;&gt; fflush(stderr) in it?<BR>
&gt;&gt;&gt;&gt;&nbsp; If the above suggestions don't narrow down the problem I will give<BR>
&gt;&gt;&gt;&gt; you a debug module (patch with some extra printfs) to help us<BR>
&gt;&gt;&gt;&gt; narrow down the problem.<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt; (PS: I looked into the code, but cannot think of anything that<BR>
&gt;&gt;&gt;&gt; might fail in your environment.)<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt; Regards,<BR>
&gt;&gt;&gt;&gt; Jayesh<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt; -----Original Message-----<BR>
&gt;&gt;&gt;&gt; From: Yu-Cheng Chou [<A HREF="mailto:cycchou@ucdavis.edu">mailto:cycchou@ucdavis.edu</A>]<BR>
&gt;&gt;&gt;&gt; Sent: Thursday, February 05, 2009 4:27 PM<BR>
&gt;&gt;&gt;&gt; To: Jayesh Krishna<BR>
&gt;&gt;&gt;&gt; Cc: mpich2-maint@mcs.anl.gov<BR>
&gt;&gt;&gt;&gt; Subject: Re: [mpich2-maint] #403: mpiexec kills the remote login<BR>
&gt;&gt;&gt;&gt; shell<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; Hi,<BR>
&gt;&gt;&gt;&gt;&gt; The debug outputs look normal (the problem could be with the part<BR>
&gt;&gt;&gt;&gt;&gt; of the code at mpiexec exit() which has no dbg statements). I have<BR>
&gt;&gt;&gt;&gt;&gt; added this to our bug tracking list.<BR>
&gt;&gt;&gt;&gt;&gt; Meanwhile,<BR>
&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt; #&nbsp; Can you send us your &quot;.smpd&quot; config file ?<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt; The &quot;.smpd&quot; file only contains one line of statement as follows.<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt; phrase=123<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt; #&nbsp; Did you modify the MPICH2 code to run on Korbet (Please send us<BR>
&gt;&gt;&gt;&gt;&gt; your configure command &amp; any env settings set to configure/make<BR>
&gt;&gt;&gt;&gt;&gt; MPICH2)?<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt; I did not modify the MPICH2 source code.<BR>
&gt;&gt;&gt;&gt; The configure command that I used is listed below.<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt; ./configure LDFLAGS=-L/tmp/korebot/mpich2-1.0.8/korebot_openssl/lib<BR>
&gt;&gt;&gt;&gt; --host=arm-linux --with-cross=crosstype --with-pm=smpd<BR>
&gt;&gt;&gt;&gt; --with-mpe=no --disable-f90 --disable-f77 --disable-cxx<BR>
&gt;&gt;&gt;&gt; --prefix=/tmp/korebot/mpich2-1.0.8/korebot_mpich2<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt; The &quot;korebot_openssl/lib&quot; contains the libraries needed for<BR>
&gt;&gt;&gt;&gt; building smpd.<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt; The content of the file &quot;crosstype&quot; is listed below.<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt; CROSS_SIZEOF_FLOAT_INT=8<BR>
&gt;&gt;&gt;&gt; CROSS_SIZEOF_DOUBLE_INT=12<BR>
&gt;&gt;&gt;&gt; CROSS_SIZEOF_LONG_INT=8<BR>
&gt;&gt;&gt;&gt; CROSS_SIZEOF_SHORT_INT=8<BR>
&gt;&gt;&gt;&gt; CROSS_SIZEOF_2_INT=8<BR>
&gt;&gt;&gt;&gt; CROSS_SIZEOF_LONG_DOUBLE_INT=16<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt; Thank you<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; -----Original Message-----<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; From: Yu-Cheng Chou [<A HREF="mailto:cycchou@ucdavis.edu">mailto:cycchou@ucdavis.edu</A>]&nbsp; &gt; Sent:<BR>
&gt;&gt;&gt;&gt;&gt; Wednesday, February 04, 2009 2:32 PM&nbsp; &gt; To: Jayesh Krishna&nbsp; &gt; Cc:<BR>
&gt;&gt;&gt;&gt;&gt; mpich-discuss@mcs.anl.gov&nbsp; &gt; Subject: Re: [mpich-discuss] mpiexec<BR>
&gt;&gt;&gt;&gt;&gt; kills the remote login shell&nbsp; &gt;&nbsp; &gt; &gt;&nbsp; Hi,<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&nbsp;&nbsp; Does smpd abort when you run your MPI job ?<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; No.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt; Regards,<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt; Jayesh<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt; -----Original Message-----<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt; From: Yu-Cheng Chou [<A HREF="mailto:cycchou@ucdavis.edu">mailto:cycchou@ucdavis.edu</A>]&nbsp; &gt; &gt; Sent:<BR>
&gt;&gt;&gt;&gt;&gt; Wednesday, February 04, 2009 1:56 PM&nbsp; &gt; &gt; To: Jayesh Krishna&nbsp; &gt; &gt; Cc:<BR>
&gt;&gt;&gt;&gt;&gt; mpich-discuss@mcs.anl.gov&nbsp; &gt; &gt; Subject: Re: [mpich-discuss]<BR>
&gt;&gt;&gt;&gt;&gt; mpiexec kills the remote login shell&nbsp; &gt; &gt;&nbsp; &gt; &gt; Hi&nbsp; &gt; &gt;&nbsp; &gt; &gt; I can<BR>
&gt;&gt;&gt;&gt;&gt; cross-compile the program and then simply run the&nbsp; &gt; executable on<BR>
&gt;&gt;&gt;&gt;&gt; &gt;<BR>
&gt;&gt;&gt;&gt;&gt; &gt; Korebot with no errors.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt; Hi,<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&nbsp; Can you try running (without mpiexec) a simple C program<BR>
&gt;&gt;&gt;&gt;&gt; with<BR>
&gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt; exit(-1) on Korebot ?<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt; ========================================<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt; #include &lt;stdlib.h&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt; int main(int argc, char *argv[])&nbsp; &gt; &gt;&gt; {<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&nbsp;&nbsp;&nbsp;&nbsp; exit(-1);<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt; }<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt; ========================================<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt; Regards,<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt; Jayesh<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt; ________________________________&nbsp; &gt; &gt;&gt; From:<BR>
&gt;&gt;&gt;&gt;&gt; mpich-discuss-bounces@mcs.anl.gov&nbsp; &gt; &gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt; [<A HREF="mailto:mpich-discuss-bounces@mcs.anl.gov">mailto:mpich-discuss-bounces@mcs.anl.gov</A>] On Behalf Of Jayesh&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt; &gt;&gt; Krishna&nbsp; &gt; &gt;&gt; Sent: Wednesday, February 04, 2009 1:04 PM&nbsp; &gt; &gt;&gt; To:<BR>
&gt;&gt;&gt;&gt;&gt; 'Yu-Cheng Chou'<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt; Cc: mpich-discuss@mcs.anl.gov&nbsp; &gt; &gt;&gt; Subject: Re:<BR>
&gt;&gt;&gt;&gt;&gt; [mpich-discuss] mpiexec kills the remote login shell&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&nbsp; Hi,<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&nbsp;&nbsp; Can you also attach the corresponding smpd debug output ?<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt; Regards,<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt; Jayesh<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt; -----Original Message-----<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt; From: Yu-Cheng Chou [<A HREF="mailto:cycchou@ucdavis.edu">mailto:cycchou@ucdavis.edu</A>]&nbsp; &gt; &gt;&gt; Sent:<BR>
&gt;&gt;&gt;&gt;&gt; Wednesday, February 04, 2009 1:02 PM&nbsp; &gt; &gt;&gt; To: Jayesh Krishna&nbsp; &gt; &gt;&gt; Cc:<BR>
&gt;&gt;&gt;&gt;&gt; mpich-discuss@mcs.anl.gov&nbsp; &gt; &gt;&gt; Subject: Re: [mpich-discuss]<BR>
&gt;&gt;&gt;&gt;&gt; mpiexec kills the remote login shell&nbsp; &gt; &gt;&gt;&nbsp; &gt; &gt;&gt; Hi,&nbsp; &gt; &gt;&gt;&nbsp; &gt; &gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt; Firstly, the previously attached mpiexec verbose output is&nbsp; &gt; a<BR>
&gt;&gt;&gt;&gt;&gt; wrong one.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt; I've attached the correct one to this email.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt; Secondly, I want to point out that as long as mpiexec is<BR>
&gt;&gt;&gt;&gt;&gt; initiated&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; from Korebot to run a program, no matter it's a MPI or non-MPI&nbsp;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; &gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; &gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt; program, no matter the program can be found or not, as soon as&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt; &gt;&gt; mpiexec is finished, the ssh connection to Korebot will be gone.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt; Thank you<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; Hi,<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&nbsp;&nbsp; The mpiexec output shows the following error when<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; running hellow,<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; ==================<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; Unable to exec 'hello' on korebot&nbsp; &gt; &gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; Error 2 -<BR>
&gt;&gt;&gt;&gt;&gt; No such file or directory&nbsp; &gt; &gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; ==================&nbsp; &gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&nbsp;&nbsp; Please provide the debug output of smpd (smpd -d 2&gt;&amp;1 | tee<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; smpd.out) along with mpiexec (mpiexec -verbose -n 2&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt; ./hellow<BR>
&gt;&gt;&gt;&gt;&gt; 2&gt;&amp;1<BR>
&gt;&gt;&gt;&gt;&gt; |&nbsp; &gt; &gt;&gt;&gt; tee mpiexec.out).<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; #&nbsp; Can you run simple C programs (without using mpiexec)&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt; on Korbet ?<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; #&nbsp; Is the ssh connection aborted when you run non-MPI<BR>
&gt;&gt;&gt;&gt;&gt; programs<BR>
&gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt; (mpiexec -n 2&nbsp; &gt; &gt;&gt;&gt; hostname) ?<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; #&nbsp; Can you send us your &quot;.smpd&quot; config file ?<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; #&nbsp; Did you modify the MPICH2 code to run on Korbet&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt; (Please send us&nbsp; &gt; &gt;&gt;&gt; your configure command &amp; any env settings<BR>
&gt;&gt;&gt;&gt;&gt; set to&nbsp; &gt; configure/make MPICH2)?<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; Regards,<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; Jayesh<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; ________________________________&nbsp; &gt; &gt;&gt;&gt; From:<BR>
&gt;&gt;&gt;&gt;&gt; mpich-discuss-bounces@mcs.anl.gov&nbsp; &gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt; [<A HREF="mailto:mpich-discuss-bounces@mcs.anl.gov">mailto:mpich-discuss-bounces@mcs.anl.gov</A>] On Behalf Of Jayesh&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt; Krishna&nbsp; &gt; &gt;&gt;&gt; Sent: Wednesday, February 04, 2009 8:41 AM&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt; To:<BR>
&gt;&gt;&gt;&gt;&gt; 'Yu-Cheng Chou'<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; Cc: mpich-discuss@mcs.anl.gov&nbsp; &gt; &gt;&gt;&gt; Subject: Re:<BR>
&gt;&gt;&gt;&gt;&gt; [mpich-discuss] mpiexec kills the remote login shell&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&nbsp; Hi,<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&nbsp;&nbsp; I will take a look at the debug logs and get back to you.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; Meanwhile, can you run simple C programs without using<BR>
&gt;&gt;&gt;&gt;&gt; mpiexec on&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Korbet ?<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&nbsp;&nbsp; MPICH2 currently does not support heterogeneous systems (So<BR>
&gt;&gt;&gt;&gt;&gt; you<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; won't be able to run your MPI job across ARM &amp; other&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt; architectures).<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; Regards,<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; Jayesh<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; -----Original Message-----<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; From: Yu-Cheng Chou [<A HREF="mailto:cycchou@ucdavis.edu">mailto:cycchou@ucdavis.edu</A>]&nbsp; &gt; &gt;&gt;&gt; Sent:<BR>
&gt;&gt;&gt;&gt;&gt; Tuesday, February 03, 2009 7:52 PM&nbsp; &gt; &gt;&gt;&gt; To: Jayesh Krishna&nbsp; &gt; &gt;&gt;&gt; Cc:<BR>
&gt;&gt;&gt;&gt;&gt; mpich-discuss@mcs.anl.gov&nbsp; &gt; &gt;&gt;&gt; Subject: Re: [mpich-discuss]<BR>
&gt;&gt;&gt;&gt;&gt; mpiexec kills the remote login shell&nbsp; &gt; &gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt; # Can you run<BR>
&gt;&gt;&gt;&gt;&gt; non-MPI programs using mpiexec (mpiexec -n&nbsp; &gt; 2 hostname) ?<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; Yes.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt; # Can you compile and run the hello world program&nbsp; &gt; &gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt; (examples/hellow.c) provided with MPICH2 (mpiexec -n 2 ./hellow)?<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; Yes.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt; # How did you start smpd (the command used to start&nbsp; &gt; smpd) ?<BR>
&gt;&gt;&gt;&gt;&gt; How did&nbsp; &gt; &gt;&gt;&gt;&gt; you run your MPI job (the command used to run your<BR>
&gt;&gt;&gt;&gt;&gt; job)?<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; I have a &quot;.smpd&quot; file containing one line of information,&nbsp;<BR>
&gt;&gt;&gt;&gt;&gt; &gt; which is&nbsp; &gt; &gt;&gt;&gt; &quot;phrase=123&quot;.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; Thus, I started smpd using &quot;smpd -s&quot;.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; Then I used &quot;mpiexec -n 1 hellow&quot; to run hellow on Korebot.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt; # How did you find that mpiexec kills the sshd process (We<BR>
&gt;&gt;&gt;&gt;&gt; &gt;<BR>
&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; typically ssh to unix machines and run mpiexec without&nbsp; &gt; any<BR>
&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; problems) ?<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; I logged in Korebot with two terminals.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; &gt;From #1 terminal, I checked all the processes running on<BR>
&gt;&gt;&gt;&gt;&gt; Korebot.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; &gt;From #2 terminal, I started smpd and run hellow using&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt; the commands&nbsp; &gt; &gt;&gt;&gt; mentioned above.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; After hellow was finished, the connection to Korebot via&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt; #2 terminal&nbsp; &gt; &gt;&gt;&gt; was closed.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; &gt;From #1 terminal, I knew that the sshd process&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt; associated with<BR>
&gt;&gt;&gt;&gt;&gt; #2&nbsp; &gt; &gt;&gt;&gt; &gt;terminal&nbsp; &gt; &gt;&gt;&gt; was gone.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt;&nbsp; Can you run smpd/mpiexec in debug mode and provide us<BR>
&gt;&gt;&gt;&gt;&gt; with the&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; outputs (smpd -d / mpiexec -n 2 -verbose hostname) ?<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; The first attached text file is the output from running<BR>
&gt;&gt;&gt;&gt;&gt; hellow in&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; mpiexec's verbose mode.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; There is another issue.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; This time, I used two machines. One is Korebot as&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt; mentioned above,&nbsp; &gt; &gt;&gt;&gt; and the other is a laptop running Ubuntu<BR>
&gt;&gt;&gt;&gt;&gt; Linux OS.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; I started smpd with the same &quot;.smpd&quot; file and command as&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt; mentioned&nbsp; &gt; &gt;&gt;&gt; above both on Korebot and the lap top.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; There is a machine file called &quot;hostfile&quot; on Korebot. The<BR>
&gt;&gt;&gt;&gt;&gt; file<BR>
&gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt; contains the following information about the name of the&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt; two machines.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; korebot<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; shrimp<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; Then from Korebot, I ran cpi using the following command.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; mpiexec -machinefile ./hostfile -verbose -n 2 cpi&nbsp; &gt; &gt;&gt;&gt;&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt; &gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; But the value of pi is a huge number. I think it is related to<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt; &quot;double type variables&quot; being transferred between&nbsp; &gt; processes<BR>
&gt;&gt;&gt;&gt;&gt; running<BR>
&gt;&gt;&gt;&gt;&gt; &gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; on an ARM-based Linux and a general Linux machines.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt; The second attached text file is the output from running<BR>
&gt;&gt;&gt;&gt;&gt; cpi in<BR>
&gt;&gt;&gt;&gt;&gt; &gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; mpiexec's verbose mode.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt; I am cross-compiling mpich2-1.0.8 with smpd for Khepera&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt; III mobile&nbsp; &gt; &gt;&gt;&gt;&gt; robot.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt; This mobile robot has a Korebot board which is an<BR>
&gt;&gt;&gt;&gt;&gt; ARM-based<BR>
&gt;&gt;&gt;&gt;&gt; &gt;<BR>
&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; computer with a Linux operating system.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt; The cross-compilation was fine.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt; Firstly, I logged in to Korebot through ssh.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt; Secondly, I started smpd.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt; Thirdly, I ran mpiexec to execute an MPI program (cpi)&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt; that comes&nbsp; &gt; &gt;&gt;&gt;&gt; with the package.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt; The result was correct, but when mpiexec was finished, the<BR>
&gt;&gt;&gt;&gt;&gt; ssh<BR>
&gt;&gt;&gt;&gt;&gt; &gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; connection to the Korebot was closed.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt; I found that mpiexec kills the sshd process through which<BR>
&gt;&gt;&gt;&gt;&gt; I was&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; remotely connected to Korebot.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt; I've been looking for the cause, but still have not&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt; found any clues.<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt; Could you give me any ideas to solve this problem?<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt; Thank you,<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt; Yu-Cheng<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt; &gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; &gt;<BR>
&gt;&gt;&gt;&gt;&gt;&nbsp; }}}<BR>
&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt; --<BR>
&gt;&gt;&gt;&gt;&gt; Ticket URL: &lt;<A HREF="https://trac.mcs.anl.gov/projects/mpich2/ticket/403">https://trac.mcs.anl.gov/projects/mpich2/ticket/403</A>&gt;<BR>
&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;<BR>
&gt;&gt;<BR>
&gt;<BR>
</FONT>
</P>

</BODY>
</HTML>