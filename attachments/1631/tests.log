***** ex2: -m 219 -n 313 -ksp_type gmres -sub_pc_type ilu -log_summary
Norm of error 0.0051934 iterations 568
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./ex2 on a arch-linu named bblogin2 with 2 processors, by chan Fri Jul 27 14:57:01 2012
Using Petsc Release Version 3.2.0, Patch 6, Wed Jan 11 09:28:45 CST 2012 

                         Max       Max/Min        Avg      Total 
Time (sec):           7.438e+00      1.00000   7.438e+00
Objects:              5.800e+01      1.00000   5.800e+01
Flops:                1.666e+09      1.00003   1.666e+09  3.332e+09
Flops/sec:            2.240e+08      1.00003   2.240e+08  4.480e+08
Memory:               2.169e+07      1.00001              4.338e+07
MPI Messages:         5.890e+02      1.00000   5.890e+02  1.178e+03
MPI Message Lengths:  1.471e+06      1.00000   2.498e+03  2.942e+06
MPI Reductions:       1.296e+04      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 7.3591e+00  98.9%  3.3318e+09 100.0%  1.174e+03  99.7%  2.495e+03       99.9%  1.294e+04  99.9% 
 1:        Assembly: 7.8372e-02   1.1%  0.0000e+00   0.0%  4.000e+00   0.3%  2.132e+00        0.1%  1.800e+01   0.1% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %f - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------


      ##########################################################
      #                                                        #
      #                          WARNING!!!                    #
      #                                                        #
      #   This code was compiled with a debugging option,      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %f %M %L %R  %T %f %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              587 1.0 2.1479e+00 1.0 1.80e+08 1.0 1.2e+03 2.5e+03 0.0e+00 29 11100100  0  29 11100100  0   168
MatSolve             587 1.0 1.7065e+00 1.0 1.80e+08 1.0 0.0e+00 0.0e+00 0.0e+00 23 11  0  0  0  23 11  0  0  0   211
MatLUFactorNum         1 1.0 2.0925e-02 1.0 3.06e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    29
MatILUFactorSym        1 1.0 1.2908e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       1 1.0 1.0731e-06 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         1 1.0 5.7379e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            1 1.0 3.0319e-06 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetSubMatrice       1 1.0 1.6329e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         1 1.0 8.3650e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot              568 1.0 1.1086e+00 1.0 6.02e+08 1.0 0.0e+00 0.0e+00 5.7e+02 15 36  0  0  4  15 36  0  0  4  1085
VecNorm              588 1.0 3.9453e-01 1.0 4.03e+07 1.0 0.0e+00 0.0e+00 5.9e+02  5  2  0  0  5   5  2  0  0  5   204
VecScale             587 1.0 1.3005e-01 1.0 2.01e+07 1.0 0.0e+00 0.0e+00 0.0e+00  2  1  0  0  0   2  1  0  0  0   309
VecCopy               19 1.0 2.0850e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               608 1.0 3.7601e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecAXPY               38 1.0 1.1800e-02 1.0 2.60e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   441
VecMAXPY             587 1.0 1.6810e+00 1.0 6.41e+08 1.0 0.0e+00 0.0e+00 0.0e+00 23 38  0  0  0  23 38  0  0  0   762
VecScatterBegin      587 1.0 7.0161e-03 1.0 0.00e+00 0.0 1.2e+03 2.5e+03 0.0e+00  0  0100100  0   0  0100100  0     0
VecScatterEnd        587 1.0 6.0262e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize         587 1.0 5.2668e-01 1.0 6.04e+07 1.0 0.0e+00 0.0e+00 5.9e+02  7  4  0  0  5   7  4  0  0  5   229
KSPGMRESOrthog       568 1.0 2.7034e+00 1.0 1.20e+09 1.0 0.0e+00 0.0e+00 9.3e+03 36 72  0  0 72  37 72  0  0 72   890
KSPSetup               2 1.0 9.9816e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 7.3384e+00 1.0 1.67e+09 1.0 1.2e+03 2.5e+03 1.3e+04 99100 99100100 100100100100100   454
PCSetUp                2 1.0 5.9586e-02 1.0 3.06e+05 1.0 0.0e+00 0.0e+00 1.4e+01  1  0  0  0  0   1  0  0  0  0    10
PCSetUpOnBlocks      588 1.0 1.8151e+00 1.0 1.80e+08 1.0 0.0e+00 0.0e+00 2.4e+03 24 11  0  0 18  25 11  0  0 18   199
PCApply              587 1.0 1.7765e+00 1.0 1.80e+08 1.0 0.0e+00 0.0e+00 2.3e+03 24 11  0  0 18  24 11  0  0 18   203

--- Event Stage 1: Assembly

MatAssemblyBegin       1 1.0 6.1493e-04 9.3 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0 11     0
MatAssemblyEnd         1 1.0 1.3063e-02 1.0 0.00e+00 0.0 4.0e+00 6.3e+02 1.6e+01  0  0  0  0  0  17  0100100 89     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     5              5     10412500     0
              Vector    39             40     10481448     0
      Vector Scatter     0              1         1036     0
           Index Set     4              4       140064     0
       Krylov Solver     2              2        19360     0
      Preconditioner     2              2         1840     0
              Viewer     1              0            0     0

--- Event Stage 1: Assembly

              Vector     2              1         1496     0
      Vector Scatter     1              0            0     0
           Index Set     2              2         1480     0
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 1.57356e-06
Average time for zero size MPI_Send(): 4.76837e-07
#PETSc Option Table entries:
-ksp_type gmres
-log_summary
-m 219
-n 313
-sub_pc_type ilu
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8
Configure run at: Wed May 23 15:04:11 2012
Configure options: -download-f-blas-lapack=1 --with-mpi-dir=/disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel --with-debugging=1 --prefix=/disk/chan/10x10/CODES/petsc/install_petsc-3.2-p6_intel
-----------------------------------------
Libraries compiled on Wed May 23 15:04:11 2012 on bblogin1 
Machine characteristics: Linux-2.6.32-34-generic-x86_64-with-Ubuntu-10.04-lucid
Using PETSc directory: /disk/chan/10x10/CODES/petsc/petsc-3.2-p6
Using PETSc arch: arch-linux2-c-debug
-----------------------------------------

Using C compiler: /disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel/bin/mpicc  -wd1572 -Qoption,cpp,--extended_float_type -g  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: /disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel/bin/mpif90  -g   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/arch-linux2-c-debug/include -I/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/include -I/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/include -I/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/arch-linux2-c-debug/include -I/disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel/include
-----------------------------------------

Using C linker: /disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel/bin/mpicc
Using Fortran linker: /disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel/bin/mpif90
Using libraries: -Wl,-rpath,/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/arch-linux2-c-debug/lib -L/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/arch-linux2-c-debug/lib -lpetsc -lX11 -lpthread -Wl,-rpath,/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/arch-linux2-c-debug/lib -L/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/arch-linux2-c-debug/lib -lflapack -lfblas  -lPEPCF90 -ldl -L/disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel/lib -lmpich -lopa -lmpl -lrt -lpthread -L/soft/intel/11.0.081/lib/intel64 -L/usr/lib/gcc/x86_64-linux-gnu/4.4.3 -limf -lsvml -lipgo -ldecimal -lirc -lgcc_s -lirc_s -lmpichf90 -lifport -lifcore -lm -lm -ldl -lmpich -lopa -lmpl -lrt -lpthread -limf -lsvml -lipgo -ldecimal -lirc -lgcc_s -lirc_s -ldl 
-----------------------------------------




***** ex2: -m 219 -n 313 -ksp_type bcgs -sub_pc_type ilu -log_summary
Norm of error 0.0046454 iterations 121
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./ex2 on a arch-linu named bblogin2 with 2 processors, by chan Fri Jul 27 14:57:03 2012
Using Petsc Release Version 3.2.0, Patch 6, Wed Jan 11 09:28:45 CST 2012 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.153e+00      1.00000   2.153e+00
Objects:              3.000e+01      1.00000   3.000e+01
Flops:                2.410e+08      1.00003   2.410e+08  4.820e+08
Flops/sec:            1.119e+08      1.00003   1.119e+08  2.239e+08
Memory:               1.422e+07      1.00002              2.844e+07
MPI Messages:         2.450e+02      1.00000   2.450e+02  4.900e+02
MPI Message Lengths:  6.097e+05      1.00000   2.489e+03  1.219e+06
MPI Reductions:       2.485e+03      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 2.0742e+00  96.3%  4.8198e+08 100.0%  4.860e+02  99.2%  2.484e+03       99.8%  2.466e+03  99.2% 
 1:        Assembly: 7.8828e-02   3.7%  0.0000e+00   0.0%  4.000e+00   0.8%  5.127e+00        0.2%  1.800e+01   0.7% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %f - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------


      ##########################################################
      #                                                        #
      #                          WARNING!!!                    #
      #                                                        #
      #   This code was compiled with a debugging option,      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %f %M %L %R  %T %f %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              243 1.0 8.7198e-01 1.0 7.47e+07 1.0 4.9e+02 2.5e+03 0.0e+00 40 31 99100  0  42 31100100  0   171
MatSolve             243 1.0 7.0613e-01 1.0 7.45e+07 1.0 0.0e+00 0.0e+00 0.0e+00 33 31  0  0  0  34 31  0  0  0   211
MatLUFactorNum         1 1.0 2.0853e-02 1.0 3.06e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0    29
MatILUFactorSym        1 1.0 1.2808e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatAssemblyBegin       1 1.0 1.0219e-06 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         1 1.0 5.7610e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            1 1.0 3.9252e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetSubMatrice       1 1.0 1.6397e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatGetOrdering         1 1.0 8.3459e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecDot               242 1.0 5.1416e-02 1.0 1.66e+07 1.0 0.0e+00 0.0e+00 2.4e+02  2  7  0  0 10   2  7  0  0 10   645
VecDotNorm2          121 1.0 6.1602e-02 1.0 1.66e+07 1.0 0.0e+00 0.0e+00 1.2e+02  3  7  0  0  5   3  7  0  0  5   539
VecNorm              123 1.0 8.1449e-02 1.0 8.43e+06 1.0 0.0e+00 0.0e+00 1.2e+02  4  3  0  0  5   4  3  0  0  5   207
VecCopy                2 1.0 2.2291e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               247 1.0 1.2206e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
VecAXPY                1 1.0 3.1602e-04 1.0 6.85e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   434
VecAXPBYCZ           242 1.0 1.1292e-01 1.0 3.32e+07 1.0 0.0e+00 0.0e+00 0.0e+00  5 14  0  0  0   5 14  0  0  0   588
VecWAXPY             242 1.0 8.5152e-02 1.0 1.66e+07 1.0 0.0e+00 0.0e+00 0.0e+00  4  7  0  0  0   4  7  0  0  0   390
VecScatterBegin      243 1.0 2.8837e-03 1.3 0.00e+00 0.0 4.9e+02 2.5e+03 0.0e+00  0  0 99100  0   0  0100100  0     0
VecScatterEnd        243 1.0 2.3523e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetup               2 1.0 1.1322e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 2.0570e+00 1.0 2.41e+08 1.0 4.8e+02 2.5e+03 2.4e+03 96100 99 99 98  99100100100 99   234
PCSetUp                2 1.0 5.9486e-02 1.0 3.06e+05 1.0 0.0e+00 0.0e+00 1.4e+01  3  0  0  0  1   3  0  0  0  1    10
PCSetUpOnBlocks      244 1.0 7.7223e-01 1.0 7.49e+07 1.0 0.0e+00 0.0e+00 9.8e+02 36 31  0  0 39  37 31  0  0 40   194
PCApply              243 1.0 7.3136e-01 1.0 7.45e+07 1.0 0.0e+00 0.0e+00 9.7e+02 34 31  0  0 39  35 31  0  0 39   204

--- Event Stage 1: Assembly

MatAssemblyBegin       1 1.0 1.5431e-0321.4 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   1  0  0  0 11     0
MatAssemblyEnd         1 1.0 1.3148e-02 1.0 0.00e+00 0.0 4.0e+00 6.3e+02 1.6e+01  1  0  1  0  1  17  0100100 89     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     5              5     10412500     0
              Vector    11             12      2762184     0
      Vector Scatter     0              1         1036     0
           Index Set     4              4       140064     0
       Krylov Solver     2              2         2152     0
      Preconditioner     2              2         1840     0
              Viewer     1              0            0     0

--- Event Stage 1: Assembly

              Vector     2              1         1496     0
      Vector Scatter     1              0            0     0
           Index Set     2              2         1480     0
========================================================================================================================
Average time to get PetscTime(): 0
Average time for MPI_Barrier(): 3.24249e-06
Average time for zero size MPI_Send(): 4.76837e-07
#PETSc Option Table entries:
-ksp_type bcgs
-log_summary
-m 219
-n 313
-sub_pc_type ilu
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8
Configure run at: Wed May 23 15:04:11 2012
Configure options: -download-f-blas-lapack=1 --with-mpi-dir=/disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel --with-debugging=1 --prefix=/disk/chan/10x10/CODES/petsc/install_petsc-3.2-p6_intel
-----------------------------------------
Libraries compiled on Wed May 23 15:04:11 2012 on bblogin1 
Machine characteristics: Linux-2.6.32-34-generic-x86_64-with-Ubuntu-10.04-lucid
Using PETSc directory: /disk/chan/10x10/CODES/petsc/petsc-3.2-p6
Using PETSc arch: arch-linux2-c-debug
-----------------------------------------

Using C compiler: /disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel/bin/mpicc  -wd1572 -Qoption,cpp,--extended_float_type -g  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: /disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel/bin/mpif90  -g   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/arch-linux2-c-debug/include -I/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/include -I/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/include -I/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/arch-linux2-c-debug/include -I/disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel/include
-----------------------------------------

Using C linker: /disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel/bin/mpicc
Using Fortran linker: /disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel/bin/mpif90
Using libraries: -Wl,-rpath,/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/arch-linux2-c-debug/lib -L/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/arch-linux2-c-debug/lib -lpetsc -lX11 -lpthread -Wl,-rpath,/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/arch-linux2-c-debug/lib -L/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/arch-linux2-c-debug/lib -lflapack -lfblas  -lPEPCF90 -ldl -L/disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel/lib -lmpich -lopa -lmpl -lrt -lpthread -L/soft/intel/11.0.081/lib/intel64 -L/usr/lib/gcc/x86_64-linux-gnu/4.4.3 -limf -lsvml -lipgo -ldecimal -lirc -lgcc_s -lirc_s -lmpichf90 -lifport -lifcore -lm -lm -ldl -lmpich -lopa -lmpl -lrt -lpthread -limf -lsvml -lipgo -ldecimal -lirc -lgcc_s -lirc_s -ldl 
-----------------------------------------




***** ex2: -m 219 -n 313 -ksp_type cg -sub_pc_type icc -log_summary
Norm of error 0.000258795 iterations 181
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./ex2 on a arch-linu named bblogin2 with 2 processors, by chan Fri Jul 27 14:57:05 2012
Using Petsc Release Version 3.2.0, Patch 6, Wed Jan 11 09:28:45 CST 2012 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.806e+00      1.00000   1.806e+00
Objects:              2.700e+01      1.00000   2.700e+01
Flops:                1.864e+08      1.00003   1.864e+08  3.728e+08
Flops/sec:            1.032e+08      1.00003   1.032e+08  2.064e+08
Memory:               1.244e+07      1.00001              2.487e+07
MPI Messages:         1.840e+02      1.00000   1.840e+02  3.680e+02
MPI Message Lengths:  4.570e+05      1.00000   2.484e+03  9.140e+05
MPI Reductions:       1.876e+03      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.7292e+00  95.7%  3.7278e+08 100.0%  3.640e+02  98.9%  2.477e+03       99.7%  1.857e+03  99.0% 
 1:        Assembly: 7.7153e-02   4.3%  0.0000e+00   0.0%  4.000e+00   1.1%  6.826e+00        0.3%  1.800e+01   1.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %f - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------


      ##########################################################
      #                                                        #
      #                          WARNING!!!                    #
      #                                                        #
      #   This code was compiled with a debugging option,      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %f %M %L %R  %T %f %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult              182 1.0 6.5195e-01 1.0 5.59e+07 1.0 3.6e+02 2.5e+03 0.0e+00 36 30 99100  0  38 30100100  0   172
MatSolve             182 1.0 6.1800e-01 1.0 5.58e+07 1.0 0.0e+00 0.0e+00 0.0e+00 34 30  0  0  0  36 30  0  0  0   181
MatCholFctrNum         1 1.0 2.1270e-02 1.0 3.43e+04 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     3
MatICCFactorSym        1 1.0 1.0047e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatAssemblyBegin       1 1.0 9.3505e-07 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         1 1.0 5.7650e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            1 1.0 6.9762e-06 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetSubMatrice       1 1.0 1.6396e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatGetOrdering         1 1.0 8.5701e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecTDot              362 1.0 7.5945e-02 1.0 2.48e+07 1.0 0.0e+00 0.0e+00 3.6e+02  4 13  0  0 19   4 13  0  0 19   653
VecNorm              183 1.0 1.2278e-01 1.0 1.25e+07 1.0 0.0e+00 0.0e+00 1.8e+02  7  7  0  0 10   7  7  0  0 10   204
VecCopy                2 1.0 2.1820e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               184 1.0 6.1411e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              363 1.0 1.1322e-01 1.0 2.49e+07 1.0 0.0e+00 0.0e+00 0.0e+00  6 13  0  0  0   7 13  0  0  0   440
VecAYPX              180 1.0 6.2681e-02 1.0 1.23e+07 1.0 0.0e+00 0.0e+00 0.0e+00  3  7  0  0  0   4  7  0  0  0   394
VecScatterBegin      182 1.0 1.8425e-03 1.2 0.00e+00 0.0 3.6e+02 2.5e+03 0.0e+00  0  0 99100  0   0  0100100  0     0
VecScatterEnd        182 1.0 1.6037e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetup               2 1.0 5.8910e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 1.7138e+00 1.0 1.86e+08 1.0 3.6e+02 2.5e+03 1.8e+03 95100 98 99 98  99100 99 99 99   217
PCSetUp                2 1.0 5.7373e-02 1.0 3.43e+04 1.0 0.0e+00 0.0e+00 1.8e+01  3  0  0  0  1   3  0  0  0  1     1
PCSetUpOnBlocks      183 1.0 6.7236e-01 1.0 5.59e+07 1.0 0.0e+00 0.0e+00 7.4e+02 37 30  0  0 39  39 30  0  0 40   166
PCApply              182 1.0 6.3318e-01 1.0 5.58e+07 1.0 0.0e+00 0.0e+00 7.3e+02 35 30  0  0 39  37 30  0  0 39   176

--- Event Stage 1: Assembly

MatAssemblyBegin       1 1.0 6.1410e-04 9.6 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0 11     0
MatAssemblyEnd         1 1.0 1.3137e-02 1.0 0.00e+00 0.0 4.0e+00 6.3e+02 1.6e+01  1  0  1  0  1  17  0100100 89     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     5              5      9046672     0
              Vector     8              9      1935120     0
      Vector Scatter     0              1         1036     0
           Index Set     4              4       140064     0
       Krylov Solver     2              2         2208     0
      Preconditioner     2              2         1792     0
              Viewer     1              0            0     0

--- Event Stage 1: Assembly

              Vector     2              1         1496     0
      Vector Scatter     1              0            0     0
           Index Set     2              2         1480     0
========================================================================================================================
Average time to get PetscTime(): 0
Average time for MPI_Barrier(): 2.00272e-06
Average time for zero size MPI_Send(): 4.76837e-07
#PETSc Option Table entries:
-ksp_type cg
-log_summary
-m 219
-n 313
-sub_pc_type icc
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8
Configure run at: Wed May 23 15:04:11 2012
Configure options: -download-f-blas-lapack=1 --with-mpi-dir=/disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel --with-debugging=1 --prefix=/disk/chan/10x10/CODES/petsc/install_petsc-3.2-p6_intel
-----------------------------------------
Libraries compiled on Wed May 23 15:04:11 2012 on bblogin1 
Machine characteristics: Linux-2.6.32-34-generic-x86_64-with-Ubuntu-10.04-lucid
Using PETSc directory: /disk/chan/10x10/CODES/petsc/petsc-3.2-p6
Using PETSc arch: arch-linux2-c-debug
-----------------------------------------

Using C compiler: /disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel/bin/mpicc  -wd1572 -Qoption,cpp,--extended_float_type -g  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: /disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel/bin/mpif90  -g   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/arch-linux2-c-debug/include -I/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/include -I/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/include -I/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/arch-linux2-c-debug/include -I/disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel/include
-----------------------------------------

Using C linker: /disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel/bin/mpicc
Using Fortran linker: /disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel/bin/mpif90
Using libraries: -Wl,-rpath,/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/arch-linux2-c-debug/lib -L/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/arch-linux2-c-debug/lib -lpetsc -lX11 -lpthread -Wl,-rpath,/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/arch-linux2-c-debug/lib -L/disk/chan/10x10/CODES/petsc/petsc-3.2-p6/arch-linux2-c-debug/lib -lflapack -lfblas  -lPEPCF90 -ldl -L/disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel/lib -lmpich -lopa -lmpl -lrt -lpthread -L/soft/intel/11.0.081/lib/intel64 -L/usr/lib/gcc/x86_64-linux-gnu/4.4.3 -limf -lsvml -lipgo -ldecimal -lirc -lgcc_s -lirc_s -lmpichf90 -lifport -lifcore -lm -lm -ldl -lmpich -lopa -lmpl -lrt -lpthread -limf -lsvml -lipgo -ldecimal -lirc -lgcc_s -lirc_s -ldl 
-----------------------------------------




***** ex2: -m 219 -n 313 -ksp_type bcgs -pc_type hypre -log_summary
[0]PETSC ERROR: --------------------- Error Message ------------------------------------
[0]PETSC ERROR: Unknown type. Check for miss-spelling or missing external package needed for type
 seehttp://www.mcs.anl.gov/petsc/petsc-as/documentation/installation.html#external!
[0]PETSC ERROR: Unable to find requested PC type hypre!
[0]PETSC ERROR: ------------------------------------------------------------------------
[0]PETSC ERROR: Petsc Release Version 3.2.0, Patch 6, Wed Jan 11 09:28:45 CST 2012 
[0]PETSC ERROR: See docs/changes/index.html for recent updates.
[0]PETSC ERROR: See docs/faq.html for hints about trouble shooting.
[0]PETSC ERROR: See docs/index.html for manual pages.
[0]PETSC ERROR: ------------------------------------------------------------------------
[0]PETSC ERROR: ./ex2 on a arch-linu named bblogin2 by chan Fri Jul 27 14:57:05 2012
[0]PETSC ERROR: Libraries linked from /disk/chan/10x10/CODES/petsc/install_petsc-3.2-p6_intel/lib
[0]PETSC ERROR: Configure run at Wed May 23 15:04:11 2012
[0]PETSC ERROR: Configure options -download-f-blas-lapack=1 --with-mpi-dir=/disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel --with-debugging=1 --prefix=/disk/chan/10x10/CODES/petsc/install_petsc-3.2-p6_intel
[0]PETSC ERROR: ------------------------------------------------------------------------
[0]PETSC ERROR: PCSetType() line 67 in /disk/chan/10x10/CODES/petsc/petsc-3.2-p6/src/ksp/pc/interface/pcset.c
[0]PETSC ERROR: PCSetFromOptions() line 184 in /disk/chan/10x10/CODES/petsc/petsc-3.2-p6/src/ksp/pc/interface/pcset.c
[0]PETSC ERROR: KSPSetFromOptions() line 286 in /disk/chan/10x10/CODES/petsc/petsc-3.2-p6/src/ksp/ksp/interface/itcl.c
[0]PETSC ERROR: main() line 193 in src/ksp/ksp/examples/tutorials/ex2.c
application called MPI_Abort(MPI_COMM_WORLD, 86) - process 0



***** ex2: -m 219 -n 313 -pc_type lu -ksp_type preonly -pc_factor_mat_solver_package superlu_dist -log_summary
[0]PETSC ERROR: --------------------- Error Message ------------------------------------
[0]PETSC ERROR: No support for this operation for this object type!
[0]PETSC ERROR: Matrix format mpiaij does not have a solver package superlu_dist for LU. Perhaps you must ./configure with --download-superlu_dist!
[0]PETSC ERROR: ------------------------------------------------------------------------
[0]PETSC ERROR: Petsc Release Version 3.2.0, Patch 6, Wed Jan 11 09:28:45 CST 2012 
[0]PETSC ERROR: See docs/changes/index.html for recent updates.
[0]PETSC ERROR: See docs/faq.html for hints about trouble shooting.
[0]PETSC ERROR: See docs/index.html for manual pages.
[0]PETSC ERROR: ------------------------------------------------------------------------
[0]PETSC ERROR: ./ex2 on a arch-linu named bblogin2 by chan Fri Jul 27 14:57:05 2012
[0]PETSC ERROR: Libraries linked from /disk/chan/10x10/CODES/petsc/install_petsc-3.2-p6_intel/lib
[0]PETSC ERROR: Configure run at Wed May 23 15:04:11 2012
[0]PETSC ERROR: Configure options -download-f-blas-lapack=1 --with-mpi-dir=/disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel --with-debugging=1 --prefix=/disk/chan/10x10/CODES/petsc/install_petsc-3.2-p6_intel
[0]PETSC ERROR: ------------------------------------------------------------------------
[0]PETSC ERROR: MatGetFactor() line 3948 in /disk/chan/10x10/CODES/petsc/petsc-3.2-p6/src/mat/interface/matrix.c
[0]PETSC ERROR: PCSetUp_LU() line 133 in /disk/chan/10x10/CODES/petsc/petsc-3.2-p6/src/ksp/pc/impls/factor/lu/lu.c
[0]PETSC ERROR: PCSetUp() line 819 in /disk/chan/10x10/CODES/petsc/petsc-3.2-p6/src/ksp/pc/interface/precon.c
[0]PETSC ERROR: KSPSetUp() line 260 in /disk/chan/10x10/CODES/petsc/petsc-3.2-p6/src/ksp/ksp/interface/itfunc.c
[0]PETSC ERROR: KSPSolve() line 379 in /disk/chan/10x10/CODES/petsc/petsc-3.2-p6/src/ksp/ksp/interface/itfunc.c
[0]PETSC ERROR: main() line 199 in src/ksp/ksp/examples/tutorials/ex2.c
application called MPI_Abort(MPI_COMM_WORLD, 56) - process 0



***** ex2: -m 219 -n 313 -pc_type lu -ksp_type preonly -pc_factor_mat_solver_package mumps -log_summary
[0]PETSC ERROR: --------------------- Error Message ------------------------------------
[0]PETSC ERROR: No support for this operation for this object type!
[0]PETSC ERROR: Matrix format mpiaij does not have a solver package mumps for LU. Perhaps you must ./configure with --download-mumps!
[0]PETSC ERROR: ------------------------------------------------------------------------
[0]PETSC ERROR: Petsc Release Version 3.2.0, Patch 6, Wed Jan 11 09:28:45 CST 2012 
[0]PETSC ERROR: See docs/changes/index.html for recent updates.
[0]PETSC ERROR: See docs/faq.html for hints about trouble shooting.
[0]PETSC ERROR: See docs/index.html for manual pages.
[0]PETSC ERROR: ------------------------------------------------------------------------
[0]PETSC ERROR: ./ex2 on a arch-linu named bblogin2 by chan Fri Jul 27 14:57:05 2012
[0]PETSC ERROR: Libraries linked from /disk/chan/10x10/CODES/petsc/install_petsc-3.2-p6_intel/lib
[0]PETSC ERROR: Configure run at Wed May 23 15:04:11 2012
[0]PETSC ERROR: Configure options -download-f-blas-lapack=1 --with-mpi-dir=/disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel --with-debugging=1 --prefix=/disk/chan/10x10/CODES/petsc/install_petsc-3.2-p6_intel
[0]PETSC ERROR: ------------------------------------------------------------------------
[0]PETSC ERROR: MatGetFactor() line 3948 in /disk/chan/10x10/CODES/petsc/petsc-3.2-p6/src/mat/interface/matrix.c
[0]PETSC ERROR: PCSetUp_LU() line 133 in /disk/chan/10x10/CODES/petsc/petsc-3.2-p6/src/ksp/pc/impls/factor/lu/lu.c
[0]PETSC ERROR: PCSetUp() line 819 in /disk/chan/10x10/CODES/petsc/petsc-3.2-p6/src/ksp/pc/interface/precon.c
[0]PETSC ERROR: KSPSetUp() line 260 in /disk/chan/10x10/CODES/petsc/petsc-3.2-p6/src/ksp/ksp/interface/itfunc.c
[0]PETSC ERROR: KSPSolve() line 379 in /disk/chan/10x10/CODES/petsc/petsc-3.2-p6/src/ksp/ksp/interface/itfunc.c
[0]PETSC ERROR: main() line 199 in src/ksp/ksp/examples/tutorials/ex2.c
application called MPI_Abort(MPI_COMM_WORLD, 56) - process 0



***** ex11: -m 219 -n 313 -ksp_type gmres -sub_pc_type ilu -log_summary
[0]PETSC ERROR: --------------------- Error Message ------------------------------------
[0]PETSC ERROR: This example requires complex numbers!
[0]PETSC ERROR: ------------------------------------------------------------------------
[0]PETSC ERROR: Petsc Release Version 3.2.0, Patch 6, Wed Jan 11 09:28:45 CST 2012 
[0]PETSC ERROR: See docs/changes/index.html for recent updates.
[0]PETSC ERROR: See docs/faq.html for hints about trouble shooting.
[0]PETSC ERROR: See docs/index.html for manual pages.
[0]PETSC ERROR: ------------------------------------------------------------------------
[0]PETSC ERROR: ./ex11 on a arch-linu named bblogin2 by chan Fri Jul 27 14:57:05 2012
[0]PETSC ERROR: Libraries linked from /disk/chan/10x10/CODES/petsc/install_petsc-3.2-p6_intel/lib
[0]PETSC ERROR: Configure run at Wed May 23 15:04:11 2012
[0]PETSC ERROR: Configure options -download-f-blas-lapack=1 --with-mpi-dir=/disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel --with-debugging=1 --prefix=/disk/chan/10x10/CODES/petsc/install_petsc-3.2-p6_intel
[0]PETSC ERROR: ------------------------------------------------------------------------
[0]PETSC ERROR: main() line 54 in src/ksp/ksp/examples/tutorials/ex11.c
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 0



***** ex11: -m 219 -n 313 -ksp_type bcgs -sub_pc_type ilu -log_summary
[0]PETSC ERROR: --------------------- Error Message ------------------------------------
[0]PETSC ERROR: This example requires complex numbers!
[0]PETSC ERROR: ------------------------------------------------------------------------
[0]PETSC ERROR: Petsc Release Version 3.2.0, Patch 6, Wed Jan 11 09:28:45 CST 2012 
[0]PETSC ERROR: See docs/changes/index.html for recent updates.
[0]PETSC ERROR: See docs/faq.html for hints about trouble shooting.
[0]PETSC ERROR: See docs/index.html for manual pages.
[0]PETSC ERROR: ------------------------------------------------------------------------
[0]PETSC ERROR: ./ex11 on a arch-linu named bblogin2 by chan Fri Jul 27 14:57:05 2012
[0]PETSC ERROR: Libraries linked from /disk/chan/10x10/CODES/petsc/install_petsc-3.2-p6_intel/lib
[0]PETSC ERROR: Configure run at Wed May 23 15:04:11 2012
[0]PETSC ERROR: Configure options -download-f-blas-lapack=1 --with-mpi-dir=/disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel --with-debugging=1 --prefix=/disk/chan/10x10/CODES/petsc/install_petsc-3.2-p6_intel
[0]PETSC ERROR: ------------------------------------------------------------------------
[0]PETSC ERROR: main() line 54 in src/ksp/ksp/examples/tutorials/ex11.c
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 0



***** ex11: -m 219 -n 313 -ksp_type preonly -pc_factor_mat_solver_package superlu_dist -log_summary
[0]PETSC ERROR: --------------------- Error Message ------------------------------------
[0]PETSC ERROR: This example requires complex numbers!
[0]PETSC ERROR: ------------------------------------------------------------------------
[0]PETSC ERROR: Petsc Release Version 3.2.0, Patch 6, Wed Jan 11 09:28:45 CST 2012 
[0]PETSC ERROR: See docs/changes/index.html for recent updates.
[0]PETSC ERROR: See docs/faq.html for hints about trouble shooting.
[0]PETSC ERROR: See docs/index.html for manual pages.
[0]PETSC ERROR: ------------------------------------------------------------------------
[0]PETSC ERROR: ./ex11 on a arch-linu named bblogin2 by chan Fri Jul 27 14:57:05 2012
[0]PETSC ERROR: Libraries linked from /disk/chan/10x10/CODES/petsc/install_petsc-3.2-p6_intel/lib
[0]PETSC ERROR: Configure run at Wed May 23 15:04:11 2012
[0]PETSC ERROR: Configure options -download-f-blas-lapack=1 --with-mpi-dir=/disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel --with-debugging=1 --prefix=/disk/chan/10x10/CODES/petsc/install_petsc-3.2-p6_intel
[0]PETSC ERROR: ------------------------------------------------------------------------
[0]PETSC ERROR: main() line 54 in src/ksp/ksp/examples/tutorials/ex11.c
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 0



***** ex11: -m 219 -n 313 -ksp_type preonly -pc_factor_mat_solver_package mumps -log_summary
[0]PETSC ERROR: --------------------- Error Message ------------------------------------
[0]PETSC ERROR: This example requires complex numbers!
[0]PETSC ERROR: ------------------------------------------------------------------------
[0]PETSC ERROR: Petsc Release Version 3.2.0, Patch 6, Wed Jan 11 09:28:45 CST 2012 
[0]PETSC ERROR: See docs/changes/index.html for recent updates.
[0]PETSC ERROR: See docs/faq.html for hints about trouble shooting.
[0]PETSC ERROR: See docs/index.html for manual pages.
[0]PETSC ERROR: ------------------------------------------------------------------------
[0]PETSC ERROR: ./ex11 on a arch-linu named bblogin2 by chan Fri Jul 27 14:57:06 2012
[0]PETSC ERROR: Libraries linked from /disk/chan/10x10/CODES/petsc/install_petsc-3.2-p6_intel/lib
[0]PETSC ERROR: Configure run at Wed May 23 15:04:11 2012
[0]PETSC ERROR: Configure options -download-f-blas-lapack=1 --with-mpi-dir=/disk/chan/10x10/CODES/mpich2/install_mpich2-1.4.1p1_intel --with-debugging=1 --prefix=/disk/chan/10x10/CODES/petsc/install_petsc-3.2-p6_intel
[0]PETSC ERROR: ------------------------------------------------------------------------
[0]PETSC ERROR: main() line 54 in src/ksp/ksp/examples/tutorials/ex11.c
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 0



